{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metrics_toolbox import EvaluatorBuilder, MetricEnum, ReducerEnum # You can import every the needed object from the main package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Building the Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Using the Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluatorBuilder(\n",
       "metric_specs=[\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.BINARY, requires_probs=True, requires_labels=False, requires_classes=False), reducers=(latest), class_name=None)\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MACRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(mean, min), class_name=None)\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.CLASS, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(std), class_name=A)\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.BINARY, requires_probs=True, requires_labels=False, requires_classes=False), reducers=(latest), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MACRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(mean, min), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.CLASS, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(std), class_name=A),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder = EvaluatorBuilder() # Create a new builder instance\n",
    "\n",
    "builder.add_metric(MetricEnum.ROC_AUC_BINARY) # Add a new metric, defaulting to the latest value\n",
    "\n",
    "builder.add_metric( # Add another metric with multiple reducers\n",
    "    MetricEnum.ROC_AUC_MACRO,\n",
    "    reducers=[ReducerEnum.MEAN, ReducerEnum.MIN],\n",
    ")\n",
    "\n",
    "builder.add_metric( # Add yet another metric with a class name specified (for multi-class metrics)\n",
    "    MetricEnum.ROC_AUC_CLASS,\n",
    "    reducers=[ReducerEnum.STD],\n",
    "    class_name=\"A\",\n",
    ")\n",
    "\n",
    "display(builder) # You can view the current configuration of the builder metrics\n",
    "\n",
    "evaluator = builder.build() # Execute the building process\n",
    "display(evaluator) # You can also view the built evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Using string names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.BINARY, requires_probs=True, requires_labels=False, requires_classes=False), reducers=(latest), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MACRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(mean, min), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.CLASS, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(std, minmax, latest), class_name=A),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder = EvaluatorBuilder() # Create a new builder instance\n",
    "\n",
    "builder.add_metric(\"roc_auc_binary\") # You can use string names directly, as long as they match the enum values\n",
    "\n",
    "builder.add_metric(\n",
    "    \"roc_auc_macro\",\n",
    "    reducers=[\"mean\", \"min\"],\n",
    ")\n",
    "\n",
    "builder.add_metric(\n",
    "    \"rOc_aUc_cLaSs\",                                # Names are automatically upper-cased and matched to enum values\n",
    "    reducers=[\"sTd\", \"mInMAX\", ReducerEnum.LATEST], # You can mix string and enum values\n",
    "    class_name=\"A\",\n",
    ")\n",
    "\n",
    "evaluator = builder.build() # Execute the building process\n",
    "display(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 From a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.BINARY, requires_probs=True, requires_labels=False, requires_classes=False), reducers=(latest), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MACRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(mean, min), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.CLASS, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(std), class_name=A),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MICRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(max), class_name=None),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"metrics\": [\n",
    "        {\"name\": \"roc_auc_binary\"},\n",
    "        {\"name\": \"roc_auc_macro\", \"reducers\": [\"mean\", \"min\"]},\n",
    "        {\"name\": \"roc_auc_class\", \"reducers\": [\"std\"], \"class_name\": \"A\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "builder = EvaluatorBuilder().from_dict(config) # Create builder from config. To view an example dict: help(EvaluatorBuilder.from_dict)\n",
    "\n",
    "builder.add_metric(\"roc_auc_micro\", reducers=[\"max\"]) # You can chain additional metrics after from_dict\n",
    "\n",
    "evaluator = builder.build()\n",
    "display(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.BINARY, requires_probs=True, requires_labels=False, requires_classes=False), reducers=(latest), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MACRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(mean, min), class_name=None),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.CLASS, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(std), class_name=A),\n",
       "\tMetricSpec(metric_cls=Metric(name=MetricNameEnum.ROC_AUC, scope=MetricScopeEnum.MICRO, requires_probs=True, requires_labels=False, requires_classes=True), reducers=(max), class_name=None),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = (\n",
    "    EvaluatorBuilder()\n",
    "    .add_metric(\"roc_auc_binary\")\n",
    "    .add_metric(\"roc_auc_macro\", reducers=[\"mean\", \"min\"])\n",
    "    .add_metric(\"roc_auc_class\", reducers=[\"std\"], class_name=\"A\")\n",
    "    .add_metric(\"roc_auc_micro\", reducers=[\"max\"])\n",
    ").build()  # You can chain the whole building process\n",
    "\n",
    "display(evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
