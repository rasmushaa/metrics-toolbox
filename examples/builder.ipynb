{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can import every the needed object from the main package\n",
    "from metrics_toolbox import EvaluatorBuilder, MetricEnum, ReducerEnum "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Building the Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Using the Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluatorBuilder(\n",
       "metric_specs=[\n",
       "\tMetricSpec(cls=Metric(name=roc_auc, scope=micro, type=probs), reducers=(latest))\n",
       "\tMetricSpec(cls=Metric(name=roc_auc, scope=macro, type=probs), reducers=(mean, min))\n",
       "\tMetricSpec(cls=Metric(name=roc_auc, scope=target, type=probs), reducers=(std))\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=micro, type=probs), reducers=(latest)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=macro, type=probs), reducers=(mean, min)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=target, type=probs), reducers=(std)),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Create a new builder instance\n",
    "builder = EvaluatorBuilder()\n",
    "\n",
    "# 2. Add a new metric, defaulting to the latest value\n",
    "builder.add_metric(MetricEnum.ROC_AUC_MICRO) \n",
    "\n",
    "# 3. Add another metric with multiple reducers\n",
    "builder.add_metric(\n",
    "    MetricEnum.ROC_AUC_MACRO,\n",
    "    reducers=[ReducerEnum.MEAN, ReducerEnum.MIN],\n",
    ")\n",
    "\n",
    "# 3, Add yet another metric with a class name specified (for multi-class metrics). \n",
    "# Note: Metrics support different options. See the documentation for more details.\n",
    "builder.add_metric( \n",
    "    MetricEnum.ROC_AUC_TARGET,\n",
    "    reducers=[ReducerEnum.STD],\n",
    "    target_name=\"A\",\n",
    ")\n",
    "\n",
    "# 5. You can view the current configuration of the builder metrics\n",
    "display(builder) \n",
    "\n",
    "# 6. Execute the building process\n",
    "evaluator = builder.build() \n",
    "\n",
    "# 7. You can also view the built evaluator\n",
    "display(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Using string names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=micro, type=probs), reducers=(latest)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=macro, type=probs), reducers=(mean, min)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=target, type=probs), reducers=(std, minmax, latest)),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Create a new builder instance\n",
    "builder = EvaluatorBuilder() \n",
    "\n",
    "# 2. You can use string names directly, as long as they match the enum values\n",
    "builder.add_metric(\"roc_auc_micro\") \n",
    "\n",
    "# 3. Also reducers can be specified as strings\n",
    "builder.add_metric(\n",
    "    \"roc_auc_macro\",\n",
    "    reducers=[\"mean\", \"min\"],\n",
    ")\n",
    "\n",
    "# 4. You can mix string and enum values when specifying options\n",
    "# Note: Names are automatically upper-cased and matched to enum values\n",
    "builder.add_metric(\n",
    "    \"rOc_aUc_tArGeT\",\n",
    "    reducers=[\"sTd\", \"mInMAX\", ReducerEnum.LATEST],\n",
    "    target_name=\"A\",\n",
    ")\n",
    "\n",
    "# 5. Execute the building process\n",
    "evaluator = builder.build() \n",
    "display(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 From a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2c7881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_dict in module metrics_toolbox.builder:\n",
      "\n",
      "from_dict(cfg: dict) -> 'EvaluatorBuilder' method of metrics_toolbox.builder.EvaluatorBuilder instance\n",
      "    Configure the builder from a dictionary.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    cfg : dict\n",
      "        The configuration dictionary.\n",
      "        Must contain a \"metrics\" key with a list of metric specifications.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    EvaluatorBuilder\n",
      "        The builder instance for chaining.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> cfg = {\n",
      "    ...     \"metrics\": [\n",
      "    ...         {\n",
      "    ...             \"name\": \"roc_auc_class\",\n",
      "    ...             \"reducers\": [\"mean\", \"min\"],\n",
      "    ...             \"class_name\": \"A\"\n",
      "    ...         },\n",
      "    ...     ]\n",
      "    ... }\n",
      "    >>> builder = EvaluatorBuilder().from_dict(cfg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a new builder instance\n",
    "builder = EvaluatorBuilder()\n",
    "\n",
    "# 2. See the documentation for from_dict usage\n",
    "help(builder.from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=macro, type=probs), reducers=(mean, min)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=target, type=probs), reducers=(std)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=micro, type=probs), reducers=(max)),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Create a config dictionary, according to the expected format\n",
    "config = {\n",
    "    \"metrics\": [\n",
    "        {\"name\": \"roc_auc_macro\", \"reducers\": [\"mean\", \"min\"]},\n",
    "        {\"name\": \"roc_auc_target\", \"reducers\": [\"std\"], \"target_name\": \"A\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Load the configuration into the builder\n",
    "builder.from_dict(config) \n",
    "\n",
    "# 5. You can also add more metrics after from_dict\n",
    "builder.add_metric(\"roc_auc_micro\", reducers=[\"max\"])\n",
    "\n",
    "# 6. Execute the building process\n",
    "evaluator = builder.build()\n",
    "display(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricEvaluator(\n",
       "metric_specs=[\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=macro, type=probs), reducers=(mean, min)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=target, type=probs), reducers=(std)),\n",
       "  MetricSpec(cls=Metric(name=roc_auc, scope=micro, type=probs), reducers=(max)),\n",
       "])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. You can chain the whole building process\n",
    "evaluator = (\n",
    "    EvaluatorBuilder()\n",
    "    .add_metric(\"roc_auc_macro\", reducers=[\"mean\", \"min\"])\n",
    "    .add_metric(\"roc_auc_target\", reducers=[\"std\"], target_name=\"A\")\n",
    "    .add_metric(\"roc_auc_micro\", reducers=[\"max\"])\n",
    ").build() \n",
    "\n",
    "# 2. Display the built evaluator\n",
    "display(evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
